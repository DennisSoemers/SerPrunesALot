\documentclass{article}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage[utf8]{inputenc}	% allows spanish letters required for Ontanon in references
\usepackage{algorithm}
\usepackage{algorithmic}

% Usage: \insertfigure{filename_without_extension}{caption text}{where to place}
% To refer to this figure: \reffigure{filename_without_extension}
\newcommand{\insertfigure}[3]{ 
	\begin{figure}[#3]
	\begin{center}
	\includegraphics[height = 0.45\textwidth]{#1}
	\caption{#2}
	\label{#1}
	\end{center}
	\end{figure}
}

\newcommand{\refalgorithm}[1]{Algorithm~\ref{#1}}
\newcommand{\refalgorithmline}[2]{line~\ref{#1} of \refalgorithm{#2}}
\newcommand{\refequation}[1]{Equation~\ref{#1}}
\newcommand{\reffigure}[1]{Figure~\ref{#1}}
\newcommand{\refsection}[1]{Section~\ref{#1}}
\newcommand{\reftable}[1]{Table~\ref{#1}}

\begin{document}
\begin{titlepage}
\begin{center}
{\huge \bfseries Intelligent Search \& Games: KnightThrough Agent} \\
\vspace{1cm}
{\large Dennis Soemers, Student ID: I6037052}\\
\vspace{2cm}
{\large \today}\\
\vfill
\end{center}
\textrm{Maastricht University} \\
\textrm{Department of Knowledge Engineering} \\
\textrm{Maastricht, Netherlands} \\
\textrm{Course: Intelligent Search \& Games} \\
\textrm{Course Tutors: Dr. Jos Uiterwijk \& Dr. Mark Winands}

\end{titlepage}

%-----------------------------------------------------------------------------
%		I N T R O D U C T I O N
%-----------------------------------------------------------------------------
\section{Introduction} 
This report describes the implementation of an intelligent game-playing agent for the game of \textsc{KnightThrough}. This section first provides a brief explanation of the game's rules, and then an overview of the remainder of the report.

\subsection{KnightThrough}
\textsc{KnightThrough} is a variant of the game of \textsc{BreakThrough} \cite{Breakthrough2001}, where players control Knights instead of Pawns. It is a deterministic, perfect information, two-player game played on an 8$\times$8 board, in which one player (the ''White'' player) controls White Knights, and the other player (the ''Black'' player) controls Black Knights. Each player starts the game with 16 Knights, with the initial state depicted in \reffigure{KnightThroughInitialPosition}.

\insertfigure{KnightThroughInitialPosition}{Initial Game State of \textsc{KnightThrough}}{h}

Knights are allowed to make similar moves as in Chess, but are restricted to those movements that take them closer towards the opponent's initial position. This means that White Knights must move up, and Black Knights must move down. Knights may not jump off the board, and they may not jump to squares already occupied by a piece of the same color. When a Knight lands on a square occupied by a piece of the opposing color, that piece is captured, again in the same way as in Chess.

A game of \textsc{KnightThrough} ends when one player no longer has any pieces (the player that still has pieces being the winner), or when one player has a Knight on the furthest row from that player's starting position (in which case that player wins). So the goal of the White player is to reach the top row with at least one Knight, and the goal of the Black player is to reach the bottom row with at least one Knight. Given these rules, the game is always guaranteed to end in a finite number of moves, and can therefore be labelled as a combinatorial game.

\subsection{Overview}
\refsection{SectionImplementation} provides details on the tools used for the implementation of a \textsc{KnightThrough}-playing agent, and assumptions that are made concerning the implementation in the remainder of the report. \refsection{SectionGameStateMoves} discusses the representation of Game States and Moves, and the move generation of the agent. In \refsection{SectionSearch}, the search algorithm used by the agent is described. \refsection{SectionEvalFunction} explains the agent's evaluation function. In \refsection{SectionEnhancements}, two enhancements to the search engine are discussed. Experiments that were performed during development are described in \refsection{SectionExperiments}. Finally, \refsection{SectionConclusions} concludes the report and provides ideas for future work.

%-----------------------------------------------------------------------------
%		I M P L E M E N T A T I O N   D E T A I L S
%-----------------------------------------------------------------------------
\section{Implementation Details} \label{SectionImplementation}
An intelligent agent named \textsc{Ser Prunes-A-Lot} (simply referred to as ``the agent'' in the remainder of this report) has been developed to play the game of \textsc{KnightThrough}. The agent has been written in \textsc{C++} and compiled using the \textsc{Visual C++ 2013} compiler, targeted for the x86 architecture. Version 5.3 of the \textsc{Qt} framework has been used for the development of the agent's GUI. The agent has only been tested on the Windows 7 operating system.

The agent as described in the remainder of this report uses the code from the ``AspirationSearch.h'' and ``AspirationSearch.cpp'' files in the attached source code for the AI engine. Other AI engines are included too, but they are all older implementations with fewer enhancements. These engines have no features that the AspirationSearch engine does not have, and are therefore not further described in this report.

Whenever a variable is described as having the integer type, it can be assumed to be a 32-bit integer (which is the default size of the ``int'' type with the used compiler settings). When a bigger or smaller size for integral types is used, this is specifically noted. Whenever the report mentions a collection of variables, the used data structure is the \emph{std::vector} from the Standard Template Library, unless otherwise noted.

%-----------------------------------------------------------------------------
%		G A M E   S T A T E   R E P R E S E N T A T I O N
%-----------------------------------------------------------------------------
\section{Game State \& Moves} \label{SectionGameStateMoves}
This section describes how Game States and Moves are represented in the agent's implementation. It also describes how, using these representations, the collection of legal moves is generated for any given game state.

\subsection{Game State Representation}
The agent defines a game state using five variables:
\begin{itemize}
\item \emph{blackBitboard}: A 64-bit integer, where each of the 64 bits is uniquely mapped to one of the 64 squares on the game board. Any bit that is \emph{set} (meaning that this bit is a $1$, as opposed to a $0$), means that the Black player has a Knight on the corresponding square. This representation is known as a \emph{bitboard}.
\item \emph{whiteBitboard}: An equivalent variable to \emph{blackBitboard} for the White player instead of the Black player.
\item \emph{numBlackKnights}: An integer representing the number of Knights that the Black player has. This number could be deduced from \emph{blackBitboard} by scanning it for the number of set bits, but the choice was made to also keep this counter separately for more efficient access to the number of Knights that a player has.
\item \emph{numWhiteKnights}: An equivalent variable to \emph{numBlackKnights} for the White player instead of the Black player.
\item \emph{currentPlayer}: A flag indicating which player is the player to move.
\end{itemize}

\subsection{Move Representation}
A move is defined by the following three variables:
\begin{itemize}
\item \emph{from}: An integer denoting the square that the move moves away from.
\item \emph{to}: An integer denoting the square that the move moves towards.
\item \emph{captured}: A boolean variable indicating whether or not a move is a capture move.
\end{itemize}
The \emph{from} and \emph{to} variables should always be in the range $[0, 63]$. The \emph{captured} flag is required to be able to correctly undo moves.

\subsection{Move Generation}
Upon initialization of the agent, for each of the two players, a table containing 64 entries is computed, where each entry contains a small collection (of at most 4 elements) containing the legal move targets for a certain starting square. The index used to access an entry in this table corresponds to the \emph{from} square of a move, and each integer in the collection retrieved from the table corresponds to a legal \emph{to} square.

The bitboard of the player to move can be scanned for the set bits to determine on which squares that player has Knights. These squares can then be used as index for the table of precomputed moves to obtain collections of legal move targets. Numbers with only the single bit corresponding to those move targets set can then be compared to the player's own bitboard to determine whether the moves are legal, and they can be compared to the opponent's bitboard to determine whether the moves are capture moves. Scanning for set bits in this procedure is done using \emph{De Bruijn Multiplication} \cite{DeBruijnMultiplication}.

The agent's move generator ensures that all capture moves are ordered before non-capture moves. All capture moves are ordered in such a way that capture moves farther away from that player's initial rows are considered first, and all non-capture moves are also ordered among each other to prefer moves closer towards the goal.

%-----------------------------------------------------------------------------
%		S E A R C H   A L G O R I T H M
%-----------------------------------------------------------------------------
\section{Search Algorithm} \label{SectionSearch}
This section describes the search algorithm that the agent uses to search the game tree for the next move to play in any given game state.

\subsection{Alpha-Beta Search}
The core of the search algorithm is the Alpha-Beta Search algorithm. Pseudocode of this algorithm is given in \refalgorithm{AlphaBeta}.
\begin{algorithm}
\caption{Alpha-Beta Search}
\label{AlphaBeta}
\begin{algorithmic}[1]
\REQUIRE Current game state $s$, $depth$, $\alpha$, $\beta$
	\IF{$s$ is terminal or $depth = 0$}
		\RETURN $evaluate(s)$ \label{LineEvaluation}
	\ENDIF
	\STATE $score \leftarrow -\infty$
	\FOR{$m \in s.generateMoves()$}
		\STATE $s.applyMove(m)$
		\STATE $value \leftarrow -$Alpha-Beta Search($s$, $depth - 1$, $-\beta$, $-\alpha$)
		\STATE $s.undoMove(m)$
		\IF{$value > score$}
			\STATE $score \leftarrow value$
		\ENDIF
		\IF{$score > \alpha$}
			\STATE $\alpha \leftarrow score$
		\ENDIF
		\IF{$score \geq \beta$}
			\STATE \bf{break}
		\ENDIF
	\ENDFOR
	\RETURN $score$
\end{algorithmic}
\end{algorithm}
When the agent is required to make a move, first an adapted version of \refalgorithm{AlphaBeta} is called that also keeps track of the move with the highest $score$, and returns the best move instead of returning a number. In the basic Alpha-Beta framework, this function is called with $depth = $ the desired maximum search depth, $\alpha = -\infty$, and $beta = \infty$. The $evaluate(state)$ function used in \refalgorithmline{LineEvaluation}{AlphaBeta} is a heuristic evaluation function that is further described in \refsection{SectionEvalFunction}.

\subsection{Iterative Deepening}
The basic Alpha-Beta Search algorithm as described above requires a maximum search depth to be provided. To avoid spending too little or too much time in any given turn, \emph{Iterative Deepening} is used to determine the search depth. The idea of Iterative Deepening is to start with a search depth of $1$, then start a new search with a search depth of $2$, and continue in that way for a given amount of time. See \refalgorithm{IterativeDeepening} for the pseudocode corresponding to this idea.
\begin{algorithm}
\caption{Iterative Deepening}
\label{IterativeDeepening}
\begin{algorithmic}[1]
\REQUIRE Current game state $s$
	\STATE $moveToPlay \leftarrow$ \bf{null}
	\STATE $depth \leftarrow 0$
	\STATE $M \leftarrow s.generateMoves()$
	\STATE $score \leftarrow -\infty$
	\WHILE{$timeAvailable()$}
		\STATE $depth \leftarrow depth + 1$
		\STATE $(bestMove, score) \leftarrow startAlphaBeta(s, depth, -\infty, \infty)$ \label{ItDeepeningAlphaBetaCall}
		\IF{$timeAvailable()$}
			\IF{$score = win$}
				\RETURN $bestMove$ \label{ItDeepeningReturnWin}
			\ELSIF{$score = loss$}
				\RETURN $moveToPlay$ \label{ItDeepeningReturnLoss}
			\ELSE
				\STATE $moveToPlay \leftarrow bestMove$
			\ENDIF
			\STATE $M.sort()$ \label{ItDeepeningSortMoves}
		\ENDIF
	\ENDWHILE
	\RETURN $moveToPlay$
\end{algorithmic}
\end{algorithm}

In this pseudocode, it is assumed that the call to $startAlphaBeta()$ in line \ref{ItDeepeningAlphaBetaCall} returns both the best move according to the search to the given depth, and the score corresponding to that move. In the actual implementation, the entire $startAlphaBeta()$ function was manually inlined in the \emph{Iterative Deepening} loop. Line \ref{ItDeepeningReturnWin} ensures that the entire iterative deepening procedure terminates as soon as a guaranteed win for the agent is detected. This means that the agent automatically prefers fast wins over slow wins, and does not waste any searching time once the win is guaranteed. Similarly, line \ref{ItDeepeningReturnLoss} immediately terminates the search once a loss is guaranteed. The difference is that in this case, the best move of the previous search is returned, which means that the agent will attempt to delay the loss. Line \ref{ItDeepeningSortMoves} re-orders the moves available in the root node according to the scores found in the last search, which generally results in an improved move ordering for the next search with the new depth.

The agent was simply assigned 30 seconds of searching time per turn. This amount was determined experimentally, and provides a safe margin such that the agent never spends more than 15 minutes in a full game. More complex time management strategies were not implemented.

\subsection{Aspiration Search}
The \emph{Iterative Deepening} algorithm as described above still starts every search with the largest possible search window of $(-\infty, \infty)$. Generally, a good estimate can be made of evaluations that are realistic to be obtainable, and therefore the initial window can be made smaller to increase the number of prunings. This idea is implemented using \emph{Aspiration Search} \cite{LectureSlidesWindows}.

In \emph{Aspiration Search}, before starting an Alpha-Beta Search, two variables $guess$ and $\Delta$ are determined such that it is expected that the value propagated back to the root node lies in $\langle guess - \Delta, guess + \Delta \rangle$. When starting a search, $\alpha$ and $\beta$ are respectively initialized to those two bounds. Suppose that a search returns a score to the root node equal to $v$. If $guess - \Delta < v < guess + \Delta$, the result is guaranteed to be correct and likely has been found more quickly than it would have been with a larger window. If $v \leq guess - \Delta$, the search needs to be restarted with the window $(-\infty, v)$. Similarly, if $v \geq guess + \Delta$, a re-search using the window $(v, \infty)$ is required.

The values for $guess$ and $\Delta$ are dependent on the evaluation function described in \refsection{SectionEvalFunction}, and have been experimentally determined. This is further elaborated on in \refsection{SectionExperiments}.

%-----------------------------------------------------------------------------
%		E V A L U A T I O N   F U N C T I O N
%-----------------------------------------------------------------------------
\section{Evaluation Function} \label{SectionEvalFunction}
The evaluation function used by the agent for evaluating the game state in any given leaf node is a simple linear combination of two features, named $\Delta material$ and $\Delta progression$. The value of $\Delta material$ in a game state is defined by the difference in the number of Knights between the two players. For example, if the White player is to move, and he has 2 more Knights than the Black player, then $\Delta material = 2$. If the Black player was to move in the same game state, a value of $-2$ would be assigned to $\Delta material$. 

The $\Delta progression$ feature is defined as the difference in the distance between a player's side of the board and the row of the furthest travelled Knight. See \reffigure{ExampleProgressionEval} for an example. In this state, the White player has $progression = 3$, because he has a Knight 3 rows above the bottom row. The Black player has $progression = 2$, because his furthest Knight is 2 rows below the top row. From the White player's perspective, this game state has $\Delta progression = 3 - 2 = 1$.

Given these two features, the final evaluation of any game state is given by \refequation{EqEvaluation}, with a weight of $100$ for $\Delta material$ and a weight of $35$ for $\Delta progression$.

\insertfigure{ExampleProgressionEval}{White player's $progression = 3$, Black player's $progression = 2$}{t}
\begin{equation} \label{EqEvaluation}
score = 100 \times \Delta material + 35 \times \Delta progression.
\end{equation}

In game states that are already terminal, or can be made terminal in a single move because the player to move already has a Knight within 2 rows of the goal row, \refequation{EqEvaluation} is not used. In these situations a constant representing a win or a loss is returned depending on whether or not it is the evaluating player that has already won or can win in 1 move.

%-----------------------------------------------------------------------------
%		E N H A N C E M E N T S
%-----------------------------------------------------------------------------
\section{Enhancements} \label{SectionEnhancements}
On top of the \emph{Aspiration Search} algorithm described in \refsection{SectionSearch}, two enhancements have been implemented in the agent; a \emph{Transposition Table} and the \emph{Killer Move Heuristic}.

\subsection{Transposition Table}
A \emph{Transposition Table} \cite{Greenblatt1967} has been implemented to store important search information and avoid repeating searches for game states that can be reached through multiple different move sequences. The table is implemented as a dynamically allocated array with $2^{22}$ entries. Every entry in the table has room for $2$ slots of data, where one slot contains the following variables:
\begin{itemize}
\item \emph{bestMove}: The best move to play in this game state, as found by the search process that stored this data.
\item \emph{hashValue}: A 64-bit integer that can be used as an identification code for a game state.
\item \emph{value}: The value that was assigned to this game state by the search process that stored this data.
\item \emph{depth}: An 8-bit integer storing the depth to which the search tree was explored below the node corresponding to this data.
\item \emph{valueType} An 8-bit flag to indicate whether the \emph{value} found was an exact value, a lower bound, or an upper bound.
\end{itemize}
To obtain a 64-bit hash code corresponding to a game state, the Zobrist hashing method \cite{Zobrist1970} is used. The features that are used to construct this Zobrist hash code are all combinations of squares and player colors, and a single feature representing the player to move. This is important, because it is possible to reach certain game states in different ways, such that sometimes the Black player is to move and sometimes the White player is to move. Consider for example the move sequence $\langle E2-G3, E7-C6, G3-F5, C6-A5, F5-G7\rangle$ and the sequence $\langle E2-D4, G7-E6, D4-E6, E7-C6, E6-G7, C6-A5\rangle$. After the first sequence the Black player is to move, after the second sequence the White player is to move, but apart from that difference the game states are equal.

Whenever a new node is reached during the search process, the first 22 bits of the hash code corresponding to that game state are used as an index into the \emph{Transposition Table}. The remaining 42 bits are used to check which of the 2 slots in that table entry, if any, contain the correct game state. If the correct game state is found, the data is used to speed up the search process in the following way. If the value in the table is an exact value, it is returned immediately. If it is a lower or an upper bound, $\alpha$ or $\beta$ can be set accordingly. If a search is still necessary, the best move as stored in the table is tried first.

Whenever a game state is evaluated, the hash code is used in the same way to index into the table again and add the new data. If both slots are already occupied, the replacement scheme \emph{TwoDeep} \cite{ReplacementSchemes1994} is used to determine which slot to replace.

\subsection{Killer Move Heuristic}
The second enhancement used by the agent is the \emph{Killer Move Heuristic}. During a search process, a small table is stored in memory that stores the last 2 moves for each depth level that resulted in a pruning. When the search reaches another node on the same search depth, these 2 \emph{Killer Moves} are tried after any move stored in the \emph{Transposition Table}, but before any other moves. 

This heuristic is based on the assumption that moves that were good enough to result in a pruning at the same depth, are likely to still be good moves, and therefore the heuristic improves move ordering. A small disadvantage is that the \emph{Killer Moves} may not be legal moves in certain game states at the same depth, and therefore require a move legality check. Apart from the improved move ordering, another advantage is that, in any case where a move from the \emph{Transposition Table} or a \emph{Killer Move} creates a pruning, it is not necessary to generate all the other moves.

%-----------------------------------------------------------------------------
%		E X P E R I M E N T S
%-----------------------------------------------------------------------------
\section{Experiments} \label{SectionExperiments}

\subsection{Aspiration Search Parameters}
A small number of experiments has been carried out to determine good ways to determine values for the $guess$ and $\Delta$ parameters of \emph{Aspiration Search} as described in \refsection{SectionSearch}. Three different set-ups were tried. They were all tested by letting the \emph{Aspiration Search} engine play against an \emph{Iterative Deepening} engine. Both engines had a \emph{Transposition Table}, but did not yet use the \emph{Killer Move} heuristic. The game states were also not yet implemented using bitboards, but using a less advanced array-based representation.

The first set-up was to always set $guess$ to the root node evaluation of the previous iteration during an \emph{Iterative Deepening} process, the last evaluation of the previous search at the start of a new search, or $0$ if no history was available. The $\Delta$ parameter was set to a constant value of $136$. With these settings, the \emph{Iterative Deepening} engine beat the \emph{Aspiration Search} engine due to being able to search deeper. It was noted that the \emph{Aspiration Search} engine often required re-searches in iterations using search depths in the range from $4$ to $7$.

Increasing $\Delta$ to $171$ enabled the \emph{Aspiration Search} engine engine to beat the \emph{Iterative Deepening} engine. This noticeably reduced the number of required re-searches, but the increased window size also reduces the number of extra prunings given by \emph{Aspiration Search}.

Finally, $\Delta$ was lowered to $100$, but $guess$ was changed every iteration by adding $141$ or $-141$ to compensate for the odd-even effect often seen in Alpha-Beta search. The values $141$ and $-141$ in this case were experimentally determined. With these settings, the number of re-searches was low (often only initially with a search depth of 1, and in the end-game when wins or losses were detected). The window size is also the smallest among all the tested set-ups. With these settings, the \emph{Aspiration Search} engine was also capable of beating the \emph{Iterative Deepening} engine.

\subsection{Enhanced Evaluation Function}
In an earlier version of the agent, where the Game State representation was still based on a matrix representing the board instead of the more compact bitboards, an enhanced evaluation function with a third feature was implemented and tested. This third feature, named $controlledProgression$, was similar to the $progression$ feature described in \refsection{SectionEvalFunction}, but only considered the progression of Knights on squares that could be attacked by at least as many allied Knights as the number of opposing attackers. 

Using equal search depths, and a small weight of $1$ (with the other two features having weights of $35$ and $100$), this evaluation function was capable of beating the simpler evaluation function of two features. However, at the time, this evaluation function was too expensive and reduced the search depth that was feasible within 30 seconds. Taking this search depth disadvantage into account, it was no longer able to beat the simpler evaluation function.

%-----------------------------------------------------------------------------
%		C O N C L U S I O N S   &   F U T U R E   W O R K
%-----------------------------------------------------------------------------
\section{Conclusions \& Future Work} \label{SectionConclusions}
This report describes the implementation of an intelligent agent capable of playing the game of \textsc{KnightThrough}. The search engine is based on the Alpha-Beta Search framework, enhanced with \emph{Iterative Deepening} for simple time management, and \emph{Aspiration Search}, a \emph{Transposition Table} and the \emph{Killer Move Heuristic} for reduction of the search space.

An enhanced evaluation function was tested earlier on in development, but at the time found not to provide enough benefit to compensate for the increased cost. The same enhancement could likely be implemented in a much more efficient manner using the bitboards that were used for the game state representation afterwards, but this has not yet been tested.

Another promising idea for future work would be to replace \emph{Aspiration Search} with \emph{Principal Variation Search} \cite{Marsland1982}. Additionally, forward pruning techniques such as Multi-Cut \cite{MultiCut1999} could be tested. Finally, it would be useful to perform larger numbers of experiments with a larger variety of opponents to test exactly to what extent the discussed enhancements improve the performance in the game of \textsc{KnightThrough}.

%-----------------------------------------------------------------------------
%		R E F E R E N C E S
%-----------------------------------------------------------------------------
\bibliographystyle{apacite}
\bibliography{References}

\end{document}
